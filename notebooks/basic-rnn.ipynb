{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model: \n",
      " - learns neither dependency: 0.661563238158\n",
      " - learns first dependency: 0.57497963068\n",
      " - learns both dependency: 0.454454367449\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected cross entropy loss if the model: \")\n",
    "print(\" - learns neither dependency:\", -(0.625*np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "\n",
    "# learns first dependency only ==> 0.519166\n",
    "print(\" - learns first dependency:\", -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "                                      - 0.5  * (0.8625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "# learns first dependency only ==> 0.519166\n",
    "print(\" - learns both dependency:\", -0.5 * (0.750 * np.log(0.75) + 0.25 * np.log(0.25)) \n",
    "                                      - 0.25  * (2 * 0.50 * np.log(0.5)) \n",
    "                                      -  0.25 * (0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be as simple as possible: at time step t, for \n",
    "t\n",
    "∈\n",
    "{\n",
    "0\n",
    ",\n",
    "1\n",
    ",\n",
    "…\n",
    "n\n",
    "}\n",
    "t∈{0,1,…n} the model accepts a (one-hot) binary \n",
    "X\n",
    "t\n",
    "Xt vector and a previous state vector, \n",
    "S\n",
    "t\n",
    "−\n",
    "1\n",
    "St−1, as inputs and produces a state vector, \n",
    "S\n",
    "t\n",
    "St, and a predicted probability distribution vector, \n",
    "P\n",
    "t\n",
    "Pt, for the (one-hot) binary vector \n",
    "Y\n",
    "t\n",
    "Yt.\n",
    "\n",
    "Formally, the model is:\n",
    "\n",
    "\n",
    "St=tanh(W(Xt @ St−1)+bs)\n",
    "\n",
    "Pt=softmax(USt+bp)\n",
    "<img src=\"data/BasicRNN.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual pattern for dealing with very long sequences is therefore to “truncate” our backpropagation by backpropagating errors a maximum of \n",
    "n\n",
    "n steps. We choose \n",
    "n\n",
    "n as a hyperparameter to our model, keeping in mind the trade-off: higher \n",
    "n\n",
    "n lets us capture longer term dependencies, but is more expensive computationally and memory-wise.\n",
    "A natural interpretation of backpropagating errors a maximum of \n",
    "n\n",
    "n steps means that we backpropagate every possible error \n",
    "n\n",
    "n steps. That is, if we have a sequence of length 49, and choose \n",
    "n\n",
    "=\n",
    "7\n",
    "n=7, we would backpropagate 42 of the errors the full 7 steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Truncated backpropagation] is easy to implement by feeding inputs of length [\n",
    "n\n",
    "n] at a time and doing backward pass after each iteration.”). This means that we would take our sequence of length 49, break it up into 7 sub-sequences of length 7 that we feed into the graph in 7 separate computations, and that only the errors from the 7th input in each graph are backpropagated the full 7 steps. Therefore, even if you think there are no dependencies longer than 7 steps in your data, it may still be worthwhile to use \n",
    "n\n",
    ">\n",
    "7\n",
    "n>7 so as to increase the proportion of errors that are backpropagated by 7 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/BasicRNNLabeled.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "    \n",
    "    # partition raw_data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    \n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i : batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i : batch_partition_length * (i + 1)]\n",
    "    \n",
    "    # further divide batch partition into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps : (i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps : (i + 1) * num_steps]\n",
    "        print(x.shape, y.shape)\n",
    "        yield (x, y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_inputs shape: 5\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "# RNN inputs\n",
    "\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "print(\"rnn_inputs shape:\", len(rnn_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of rnn cell\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "        #print('rnn_input shape', rnn_input.shape)\n",
    "        #print('state shape', state.shape)\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_input shape (200, 2)\n",
      "state shape (200, 4)\n",
      "rnn_input shape (200, 2)\n",
      "state shape (200, 4)\n",
      "rnn_input shape (200, 2)\n",
      "state shape (200, 4)\n",
      "rnn_input shape (200, 2)\n",
      "state shape (200, 4)\n",
      "rnn_input shape (200, 2)\n",
      "state shape (200, 4)\n"
     ]
    }
   ],
   "source": [
    "# adding rnn to graph\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "# loss and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) \n",
    "          for logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch, in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print('\\nEPOCH', idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = sess.run([losses,\n",
    "                                                                         total_loss,\n",
    "                                                                         final_state,\n",
    "                                                                         train_step], \n",
    "                                                                        feed_dict={x:X,\n",
    "                                                                                   y:Y,\n",
    "                                                                                   init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"average loss at step \", step, ' for last 250 steps: ', training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "        return training_losses\n",
    "                    \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  100  for last 250 steps:  0.625226772428\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  200  for last 250 steps:  0.531449468136\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  300  for last 250 steps:  0.522525060475\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  400  for last 250 steps:  0.521959916651\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  500  for last 250 steps:  0.525886330307\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  600  for last 250 steps:  0.522295415699\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  700  for last 250 steps:  0.52019502461\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  800  for last 250 steps:  0.522357655168\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "average loss at step  900  for last 250 steps:  0.521432018578\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n",
      "(200, 5) (200, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ca06b9358>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIRotAXEYkQhb6ISIT8fwbKdrYTU9eoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ca1477eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(1, num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in another file\n",
    "import basic_rnn\n",
    "def plot_learning_curve(num_steps, state_size=4, epochs=1):\n",
    "    global losses, total_loss, final_state, train_step, x, y, init_state\n",
    "    tf.reset_default_graph()\n",
    "    g = tf.get_default_graph()\n",
    "    losses, total_loss, final_state, train_step, x, y, init_state = basic_rnn.setup_graph(\n",
    "                                                                                g, \n",
    "                                                                                basic_rnn.RNN_config(\n",
    "                                                                                num_steps=num_steps,\n",
    "                                                                                state_size=state_size)\n",
    "                                                                                )\n",
    "    res = train_network(epochs, num_steps, state_size=state_size, verbose=False)\n",
    "    plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7ff8ff680894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-555277a34fdd>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(num_steps, state_size, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     losses, total_loss, final_state, train_step, x, y, init_state = setup_graph(\n\u001b[0m\u001b[1;32m      7\u001b[0m                                                                                 \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                                 RNN_config(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_graph' is not defined"
     ]
    }
   ],
   "source": [
    "plot_learning_curve(num_steps=1, state_size=4, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
