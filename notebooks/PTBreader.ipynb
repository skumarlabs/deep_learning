{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://mattmahoney.net/dc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url+filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print(\"Found and verified\")\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. You can get with browser')\n",
    "    \n",
    "    return filename    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'text8.zip'\n",
    "#filename, _ = urllib.request.urlretrieve(url+filename, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = req.get(url + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.stat_result(st_mode=33204, st_ino=657974, st_dev=2054, st_nlink=1, st_uid=1000, st_gid=1000, st_size=31344016, st_atime=1508346887, st_mtime=1508347534, st_ctime=1508347534)\n"
     ]
    }
   ],
   "source": [
    "print(os.stat(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "    \n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0])) # sort in descending order, x is a tuple\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    return word_to_id    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'man': 2, 'woman': 1, 'that': 1})\n",
      "dict_items([('the', 2), ('woman', 1), ('man', 2), ('that', 1)])\n",
      "('man', 'the', 'that', 'woman')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'man': 0, 'that': 2, 'the': 1, 'woman': 3}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = collections.Counter([\"the\", \"that\", \"man\", \"woman\", \"man\", \"the\"])\n",
    "print(counter)\n",
    "print(counter.items())\n",
    "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "words, _ = list(zip(*count_pairs))\n",
    "word_to_id = dict(zip(words, range(len(words))))\n",
    "print(words)\n",
    "word_to_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptb_raw_data(data_path=None):\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "    \n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    \n",
    "    vocabulary = len(word_to_id)\n",
    "    \n",
    "    return train_data, valid_data, test_data, vocabulary  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "    with tf.name_scope(\"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "        \n",
    "        data_len = tf.size(raw_data)\n",
    "        batch_len = data_len//batch_size\n",
    "        \n",
    "        data = tf.reshape(raw_data[0:batch_size*batch_len], [batch_size, batch_len])\n",
    "        epoch_size = [batch_len -1]//num_steps\n",
    "        assertion = tf.assert_positive(epoch_size, message=\"epoch_size==0, decrease batch_size or num_steps\")\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "            \n",
    "        i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "        x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "        x.set_shape([batch_size, num_steps])\n",
    "        y = tf.strided_slice(data, [0, i+num_steps+1],[batch_size, (i+1)*num_steps+1] )\n",
    "        y.set_shape([batch_size, num_steps])\n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
